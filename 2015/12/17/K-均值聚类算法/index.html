<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  
  <title>K-均值聚类算法 | witness</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="description" content="&amp;emsp;&amp;emsp;聚类是一种无监督分类，K-均值聚类可以在训练数据集中发现K个不同的簇，且每个簇的中心采用簇中所含训练数据的均值计算而成。">
<meta property="og:type" content="article">
<meta property="og:title" content="K-均值聚类算法">
<meta property="og:url" content="http://yoursite.com/2015/12/17/K-均值聚类算法/index.html">
<meta property="og:site_name" content="witness">
<meta property="og:description" content="&amp;emsp;&amp;emsp;聚类是一种无监督分类，K-均值聚类可以在训练数据集中发现K个不同的簇，且每个簇的中心采用簇中所含训练数据的均值计算而成。">
<meta property="og:image" content="http://yoursite.com/images/K-Means.png">
<meta property="og:updated_time" content="2015-12-15T12:02:44.000Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="K-均值聚类算法">
<meta name="twitter:description" content="&amp;emsp;&amp;emsp;聚类是一种无监督分类，K-均值聚类可以在训练数据集中发现K个不同的簇，且每个簇的中心采用簇中所含训练数据的均值计算而成。">
  
    <link rel="alternative" href="/atom.xml" title="witness" type="application/atom+xml">
  
  
    <link rel="icon" href="/favicon.png">
  
  <link href="//fonts.googleapis.com/css?family=Source+Code+Pro" rel="stylesheet" type="text/css">
  <link rel="stylesheet" href="/css/style.css" type="text/css">
  

</head>
<body>
  <div id="container">
    <div id="wrap">
      <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <div id="header-title" class="inner">
      <h1 id="logo-wrap">
        <a href="/" id="logo">witness</a>
      </h1>
      
        <h2 id="subtitle-wrap">
          <a href="/" id="subtitle">where you are where amazing happen</a>
        </h2>
      
    </div>
    <div id="header-inner" class="inner">
      <nav id="main-nav">
        <a id="main-nav-toggle" class="nav-icon"></a>
        
          <a class="main-nav-link" href="/">Home</a>
        
          <a class="main-nav-link" href="/archives">Archives</a>
        
      </nav>
      <nav id="sub-nav">
        
          <a id="nav-rss-link" class="nav-icon" href="/atom.xml" title="RSS Feed"></a>
        
        <a id="nav-search-btn" class="nav-icon" title="Search"></a>
      </nav>
      <div id="search-form-wrap">
        <form action="//google.com/search" method="get" accept-charset="UTF-8" class="search-form"><input type="search" name="q" results="0" class="search-form-input" placeholder="Search"><button type="submit" class="search-form-submit">&#xF002;</button><input type="hidden" name="sitesearch" value="http://yoursite.com"></form>
      </div>
    </div>
  </div>
</header>
      <div class="outer">
        <section id="main"><article id="post-K-均值聚类算法" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2015/12/17/K-均值聚类算法/" class="article-date">
  <time datetime="2015-12-17T09:51:23.000Z" itemprop="datePublished">2015-12-17</time>
</a>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/机器学习/">机器学习</a>
  </div>

  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 class="article-title" itemprop="name">
      K-均值聚类算法
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>&emsp;&emsp;聚类是一种无监督分类，K-均值聚类可以在训练数据集中发现K个不同的簇，且每个簇的中心采用簇中所含训练数据的均值计算而成。<br><a id="more"></a></p>
<h2 id="K-_u5747_u503C_u805A_u7C7B_u7B97_u6CD5"><a href="#K-_u5747_u503C_u805A_u7C7B_u7B97_u6CD5" class="headerlink" title="K-均值聚类算法"></a>K-均值聚类算法</h2><p>&emsp;&emsp;优点：简单，容易实现。<br>&emsp;&emsp;缺点：可能收敛到局部最小值，在大规模数据集上收敛较慢。</p>
<p>&emsp;&emsp;K-均值算法的工作流程为：首先，确定 $k$ 个初始点作为质心；然后将数据集中的每个点分配到相应的簇中，准确的说，就是在数据集中，为每个点计算其到各个质心的距离，并将数据点分配给距离最小的质心。完成后再更新质心为该簇的平均值。<br>&emsp;&emsp;上述过程伪代码描述如下：<br><strong>$\qquad$创建 $k$ 个点最为初始质心（通常随机选择）<br>$\qquad$当任意一个点的簇分配结果发生改变时：<br>$\qquad\qquad$遍历数据集，对每个点：<br>$\qquad\qquad\qquad$对每个质心：<br>$\qquad\qquad\qquad\qquad$计算数据点到质心的距离<br>$\qquad\qquad\qquad$将数据点分配给距离最近的质心<br>$\qquad\qquad$对每一个簇，计算该簇中所有数据点的均值作为质心</strong></p>
<h2 id="u4F7F_u7528_u540E_u5904_u7406_u6765_u63D0_u9AD8_u805A_u7C7B_u6027_u80FD"><a href="#u4F7F_u7528_u540E_u5904_u7406_u6765_u63D0_u9AD8_u805A_u7C7B_u6027_u80FD" class="headerlink" title="使用后处理来提高聚类性能"></a>使用后处理来提高聚类性能</h2><p>&emsp;&emsp;K-均值算法收敛但聚类效果较差的原因就是收敛到局部最小值，而非全局最小值。一种用于度量聚能效果的指标是<strong>SSE(Sum of Squared Error,误差平方和)</strong>。聚类的目标是在保持簇数目不变的情况下提高簇的质量。<br>&emsp;&emsp;有两种可以量化的方法：合并最近的质心，或者合并两个使得SSE增幅最小的质心。第一种思路需要计算所有质心之间的距离，然后选择距离最小的两个质心合并来是实现；第二种思路需要合并两个簇然后计算总SSE值。这必须在所有可能的两个簇上重复该过程，找出最佳的两个簇。</p>
<h2 id="u4E8C_u5206K-_u5747_u503C_u7B97_u6CD5"><a href="#u4E8C_u5206K-_u5747_u503C_u7B97_u6CD5" class="headerlink" title="二分K-均值算法"></a>二分K-均值算法</h2><p>&emsp;&emsp;为了克服K-均值算法收敛与局部最小值的问题，提出了一种称为<strong>二分K-均值算法</strong>。该算法首先将整个数据集作为一个簇，然后将该簇一分为二，之后选择其中的一个簇继续进行划分。选择哪一个簇进行划分取决于该划分是否最大程度降低SSE值。上述基于SSE的划分过程不断重复，直到满足指定的簇数。该过程伪代码描述如下：<br><strong>$\qquad$将整个数据集看成一个簇<br>$\qquad$当簇的数目小于 $k$ 时：<br>$\qquad\qquad$对于每一个簇：<br>$\qquad\qquad\qquad$计算总误差<br>$\qquad\qquad\qquad$将给定的簇进行K-均值聚类（k=2）<br>$\qquad\qquad\qquad$计算将簇一分为二后的总误差<br>$\qquad\qquad$选择使得误差最小的那个簇进行划分操作</strong></p>
<h2 id="u5B9E_u9A8C_uFF08python_u5B9E_u73B0_uFF09"><a href="#u5B9E_u9A8C_uFF08python_u5B9E_u73B0_uFF09" class="headerlink" title="实验（python实现）"></a>实验（python实现）</h2><p>&emsp;&emsp;附：数据集下载链接：<a href="http://pan.baidu.com/s/1eRzQTAM" target="_blank" rel="external">http://pan.baidu.com/s/1eRzQTAM</a><br>&emsp;&emsp;<strong>效果图</strong>：<br><img src="/images/K-Means.png" width="512" height="512" title="K-Means聚类效果图"><br>&emsp;&emsp;<strong>代码</strong>：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># -*- coding: UTF-8 -*-</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> math</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 读取数据集</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">load_dataset</span><span class="params">(file_path)</span>:</span></span><br><span class="line">    dataset = []</span><br><span class="line">    fr = open(file_path)</span><br><span class="line">    <span class="keyword">for</span> line <span class="keyword">in</span> fr.readlines():</span><br><span class="line">        line_arr = line.strip().split(<span class="string">'\t'</span>)</span><br><span class="line">        line_float = map(float, line_arr)</span><br><span class="line">        dataset.append(line_float)</span><br><span class="line">    fr.close()</span><br><span class="line">    <span class="keyword">return</span> dataset</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 计算距离</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_distEuclidean</span><span class="params">(vecA, vecB)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> math.sqrt(np.sum(np.power(vecA - vecB, <span class="number">2</span>)))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 随机生成簇的质心</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_randCenter</span><span class="params">(dataset, k)</span>:</span></span><br><span class="line">    n = dataset.shape[<span class="number">1</span>]</span><br><span class="line">    centroids = np.mat(np.zeros((k, n)))</span><br><span class="line">    <span class="keyword">for</span> j <span class="keyword">in</span> range(n):</span><br><span class="line">        minJ = min(dataset[:, j])</span><br><span class="line">        rangeJ = float(max(dataset[:, j]) - minJ)</span><br><span class="line">        centroids[:, j] = minJ + rangeJ * np.random.rand(k, <span class="number">1</span>)</span><br><span class="line">    <span class="comment"># centroids = np.mat('-0.27409069  4.06949785;-4.46276867 -0.79273077;-2.65375091 -4.00954913;-4.53611874  4.14258773')</span></span><br><span class="line">    <span class="keyword">return</span> centroids</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 普通的k-均值聚类</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">k_means</span><span class="params">(dataset, k)</span>:</span></span><br><span class="line">    m = dataset.shape[<span class="number">0</span>]</span><br><span class="line">    cluster_assment = np.mat(np.zeros((m, <span class="number">2</span>)))  <span class="comment"># 与数据集相对应,第一列保存簇,即类别,第二列保存距离,即数据到质心的距离</span></span><br><span class="line">    centroids = get_randCenter(dataset, k)  <span class="comment"># 初始化k个质心</span></span><br><span class="line">    cluster_changed = <span class="keyword">True</span></span><br><span class="line">    <span class="keyword">while</span> cluster_changed:</span><br><span class="line">        cluster_changed = <span class="keyword">False</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(m):  <span class="comment"># 遍历数据集</span></span><br><span class="line">            min_j = -<span class="number">1</span></span><br><span class="line">            min_dist = np.inf</span><br><span class="line">            <span class="keyword">for</span> j <span class="keyword">in</span> range(k):  <span class="comment"># 遍历质心,寻找最短距离</span></span><br><span class="line">                dist = get_distEuclidean(dataset[i, :], centroids[j, :])</span><br><span class="line">                <span class="keyword">if</span> min_dist &gt; dist:</span><br><span class="line">                    min_dist = dist</span><br><span class="line">                    min_j = j</span><br><span class="line">            <span class="keyword">if</span> cluster_assment[i, <span class="number">0</span>] != min_j:</span><br><span class="line">                cluster_changed = <span class="keyword">True</span></span><br><span class="line">            cluster_assment[i, :] = [min_j, min_dist ** <span class="number">2</span>]  <span class="comment"># 更新第i个数据对应的质心和与质心的距离</span></span><br><span class="line">        <span class="comment"># print centroids</span></span><br><span class="line">        <span class="keyword">for</span> cent <span class="keyword">in</span> range(k):</span><br><span class="line">            class_cent_list = (cluster_assment[:, <span class="number">0</span>].A == cent).nonzero()[<span class="number">0</span>]</span><br><span class="line">            centroids[cent, :] = np.mean(dataset[class_cent_list], axis=<span class="number">0</span>)</span><br><span class="line">    <span class="keyword">return</span> centroids, cluster_assment</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 二分k-均值聚类</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">bipartite_k_means</span><span class="params">(dataset, k)</span>:</span></span><br><span class="line">    m = dataset.shape[<span class="number">0</span>]</span><br><span class="line">    cluster_assment = np.mat(np.zeros((m, <span class="number">2</span>)))  <span class="comment"># 第一列保存所属的簇,第二列保存与该簇质心的距离</span></span><br><span class="line">    centroid_0 = np.mean(dataset, axis=<span class="number">0</span>).tolist()[<span class="number">0</span>]  <span class="comment"># 初始化整个数据集为一簇时的质心</span></span><br><span class="line">    centlist = [centroid_0]  <span class="comment"># 保存所有质心</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(m):  <span class="comment"># 遍历数据集,计算样本点到质心的距离</span></span><br><span class="line">        cluster_assment[i, <span class="number">1</span>] = get_distEuclidean(dataset[i, :], centroid_0)</span><br><span class="line">    <span class="keyword">while</span> len(centlist) &lt; k:</span><br><span class="line">        min_SSE = np.inf</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(len(centlist)):  <span class="comment"># 遍历已经聚类的簇</span></span><br><span class="line">            dataset_curr_cluster = dataset[(cluster_assment[:, <span class="number">0</span>].A == i).nonzero()[<span class="number">0</span>], :]  <span class="comment"># 将第i簇的数据集提取出来单独处理</span></span><br><span class="line">            centroid_i, cluster_split = k_means(dataset_curr_cluster, <span class="number">2</span>)</span><br><span class="line">            SSE_split = sum(cluster_split[:, <span class="number">1</span>])</span><br><span class="line">            SSE_non_split = sum(cluster_assment[(cluster_assment[:, <span class="number">0</span>].A != i).nonzero()[<span class="number">0</span>], <span class="number">1</span>])</span><br><span class="line">            <span class="keyword">if</span> (SSE_split + SSE_non_split) &lt; min_SSE:</span><br><span class="line">                min_SSE = SSE_non_split + SSE_split</span><br><span class="line">                minSSE_centroid_index = i  <span class="comment"># 记录使得误差平方和最小时的划分的母簇的序号</span></span><br><span class="line">                minSSE_cluster_split = cluster_split.copy()  <span class="comment"># 保存使得误差平方和最小时的划分的两个子簇数据集到各自质心的距离</span></span><br><span class="line">                minSSE_centroid = centroid_i  <span class="comment"># 保存使得误差平方和最小时的两个子簇的质心</span></span><br><span class="line">        <span class="comment"># 两个子簇在放入总的centlist前,需要将其簇序号分别设为centlist最后一个和当前分解的这个一边替换</span></span><br><span class="line">        minSSE_cluster_split[(minSSE_cluster_split[:, <span class="number">0</span>].A == <span class="number">1</span>).nonzero()[<span class="number">0</span>], <span class="number">0</span>] = len(centlist)</span><br><span class="line">        minSSE_cluster_split[(minSSE_cluster_split[:, <span class="number">0</span>].A == <span class="number">0</span>).nonzero()[<span class="number">0</span>], <span class="number">0</span>] = minSSE_centroid_index</span><br><span class="line">        cluster_assment[(cluster_assment[:, <span class="number">0</span>].A == minSSE_centroid_index).nonzero()[<span class="number">0</span>], :] = minSSE_cluster_split  <span class="comment"># 替换</span></span><br><span class="line">        centlist[minSSE_centroid_index] = minSSE_centroid.tolist()[<span class="number">0</span>]  <span class="comment"># 将分解得到的子簇添加到centlist中</span></span><br><span class="line">        centlist.append(minSSE_centroid.tolist()[<span class="number">1</span>])</span><br><span class="line">    <span class="keyword">return</span> centlist, cluster_assment</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 图形化展示结果</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">plot_centroids</span><span class="params">(dataset, centroids)</span>:</span></span><br><span class="line">    fig = plt.figure()</span><br><span class="line">    ax = fig.add_subplot(<span class="number">111</span>)</span><br><span class="line">    dataset_x = dataset[:, <span class="number">0</span>].tolist()</span><br><span class="line">    dataset_y = dataset[:, <span class="number">1</span>].tolist()</span><br><span class="line">    ax.scatter(dataset_x, dataset_y, marker=<span class="string">'o'</span>, s=<span class="number">50</span>, c=<span class="string">'gray'</span>)</span><br><span class="line">    centroids_x = centroids[:, <span class="number">0</span>].tolist()</span><br><span class="line">    centroids_y = centroids[:, <span class="number">1</span>].tolist()</span><br><span class="line">    ax.scatter(centroids_x, centroids_y, marker=<span class="string">'+'</span>, s=<span class="number">200</span>, c=<span class="string">'red'</span>)</span><br><span class="line">    plt.title(<span class="string">'cluster efficiency'</span>)</span><br><span class="line">    plt.xlabel(<span class="string">'data_x'</span>)</span><br><span class="line">    plt.ylabel(<span class="string">'data_y'</span>)</span><br><span class="line">    plt.show()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">    k = <span class="number">4</span></span><br><span class="line">    dataset = load_dataset(<span class="string">"testSet.txt"</span>)</span><br><span class="line">    centroids, cluster_assment = bipartite_k_means(np.mat(dataset), k)</span><br><span class="line">    plot_centroids(np.mat(dataset), np.mat(centroids))</span><br></pre></td></tr></table></figure></p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2015/12/17/K-均值聚类算法/" data-id="ciia9902j0000zu3v1c23kmv8" class="article-share-link">Share</a>
      
      
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/K-均值/">K-均值</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/聚类/">聚类</a></li></ul>

    </footer>
  </div>
  
    
<nav id="article-nav">
  
  
    <a href="/2015/12/17/检索模型与搜索排序/" id="article-nav-older" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Older</strong>
      <div class="article-nav-title">检索模型与搜索排序</div>
    </a>
  
</nav>

  
</article>

</section>
        
          <aside id="sidebar">
  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Categories</h3>
    <div class="widget">
      <ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/搜索引擎/">搜索引擎</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/数学/">数学</a><span class="category-list-count">2</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/机器学习/">机器学习</a><span class="category-list-count">4</span></li></ul>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Tags</h3>
    <div class="widget">
      <ul class="tag-list"><li class="tag-list-item"><a class="tag-list-link" href="/tags/K-均值/">K-均值</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/凸优化/">凸优化</a><span class="tag-list-count">4</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/分类/">分类</a><span class="tag-list-count">2</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/回归/">回归</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/拉格朗日对偶性/">拉格朗日对偶性</a><span class="tag-list-count">3</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/搜索引擎/">搜索引擎</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/支持向量机/">支持向量机</a><span class="tag-list-count">3</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/聚类/">聚类</a><span class="tag-list-count">1</span></li></ul>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Tag Cloud</h3>
    <div class="widget tagcloud">
      <a href="/tags/K-均值/" style="font-size: 10px;">K-均值</a> <a href="/tags/凸优化/" style="font-size: 20px;">凸优化</a> <a href="/tags/分类/" style="font-size: 13.33px;">分类</a> <a href="/tags/回归/" style="font-size: 10px;">回归</a> <a href="/tags/拉格朗日对偶性/" style="font-size: 16.67px;">拉格朗日对偶性</a> <a href="/tags/搜索引擎/" style="font-size: 10px;">搜索引擎</a> <a href="/tags/支持向量机/" style="font-size: 16.67px;">支持向量机</a> <a href="/tags/聚类/" style="font-size: 10px;">聚类</a>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2015/12/">December 2015</a><span class="archive-list-count">7</span></li></ul>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Recents</h3>
    <div class="widget">
      <ul>
        
          <li>
            <a href="/2015/12/17/K-均值聚类算法/">K-均值聚类算法</a>
          </li>
        
          <li>
            <a href="/2015/12/17/检索模型与搜索排序/">检索模型与搜索排序</a>
          </li>
        
          <li>
            <a href="/2015/12/17/支持向量回归机/">支持向量回归机</a>
          </li>
        
          <li>
            <a href="/2015/12/17/支持向量分类机/">支持向量分类机</a>
          </li>
        
          <li>
            <a href="/2015/12/17/拉格朗日对偶性/">拉格朗日对偶性</a>
          </li>
        
      </ul>
    </div>
  </div>

  
</aside>
        
      </div>
      <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2015 Andrew Xl<br>
      Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>
    </div>
  </div>
</footer>
    </div>
    <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>
    

<script src="//ajax.googleapis.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>


  <link rel="stylesheet" href="/fancybox/jquery.fancybox.css" type="text/css">
  <script src="/fancybox/jquery.fancybox.pack.js" type="text/javascript"></script>


<script src="/js/script.js" type="text/javascript"></script>

  </div>
</body>
</html>